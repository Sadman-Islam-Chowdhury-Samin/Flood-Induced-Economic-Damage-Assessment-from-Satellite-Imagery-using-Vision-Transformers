{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1196732,"sourceType":"datasetVersion","datasetId":681625},{"sourceId":1635643,"sourceType":"datasetVersion","datasetId":966962}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===================== Install =====================\n!pip install -q segmentation-models-pytorch torchmetrics transformers timm","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Imports =====================\nimport os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom glob import glob\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\nfrom torchmetrics.classification import MulticlassAccuracy, MulticlassJaccardIndex\nfrom transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Config =====================\nNUM_CLASSES = 12\nIMAGE_SIZE = (512, 512)\nBATCH_SIZE = 4\nEPOCHS = 20\nPATIENCE = 5\nLEARNING_RATE = 2e-4\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Class Weights =====================\nclass_weights = torch.tensor([\n    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 2.0, 0.5\n]).to(DEVICE)\n\n# ===================== Loss Functions =====================\ndef focal_loss(outputs, targets, alpha=None, gamma=2.0):\n    # Ensure target shape matches outputs for cross_entropy\n    if outputs.shape[2:] != targets.shape[1:]:\n        targets = F.interpolate(targets.unsqueeze(1).float(), size=outputs.shape[2:], mode='nearest').squeeze(1).long()\n\n    ce_loss = F.cross_entropy(outputs, targets, reduction='none', weight=alpha)\n    pt = torch.exp(-ce_loss)\n    focal = ((1 - pt) ** gamma) * ce_loss\n    return focal.mean()\n\ndef dice_loss(outputs, targets, smooth=1e-6):\n    num_classes = outputs.shape[1]\n\n    # Softmax over channels\n    outputs = F.softmax(outputs, dim=1)\n\n    # One-hot encode targets\n    targets_one_hot = F.one_hot(targets, num_classes)  # [B, H, W, C]\n    targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # [B, C, H, W]\n\n    # Resize target if needed\n    if outputs.shape != targets_one_hot.shape:\n        targets_one_hot = F.interpolate(targets_one_hot, size=outputs.shape[2:], mode='nearest')\n\n    intersection = (outputs * targets_one_hot).sum(dim=(2, 3))\n    union = outputs.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n\n    dice = (2 * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\ndef combined_loss(outputs, targets):\n    fl = focal_loss(outputs, targets, alpha=class_weights)\n    dl = dice_loss(outputs, targets)\n    return fl + dl","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Classes:**\n\n* 0: urbanland\n* 1: agricultureland\n* 2: rangeland\n* 3: forestland\n* 4: water\n* 5: barrenland\n* 6: unknown\n* 7: building (Dubai)\n* 8: land_unpaved (Dubai)\n* 9: road (Dubai)\n* 10: vegetation_dubai (Dubai)\n* 11: unlabeled\n","metadata":{}},{"cell_type":"code","source":"# ===================== Dataset =====================\nrgb_to_class = {\n    (0, 255, 255): 0, (255, 255, 0): 1, (255, 0, 255): 2,\n    (0, 255, 0): 3, (0, 0, 255): 4, (255, 255, 255): 5,\n    (0, 0, 0): 6, (60, 16, 152): 7, (132, 41, 246): 8,\n    (110, 193, 228): 9, (254, 221, 58): 10, (155, 155, 155): 11\n}\nclass_to_rgb = {v: k for k, v in rgb_to_class.items()}\n\nclass SatelliteDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, augment=False):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.augment = augment\n        self.transform = T.Compose([\n            T.Resize(IMAGE_SIZE),\n            T.ToTensor()\n        ])\n        self.aug = T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomVerticalFlip(),\n            T.RandomRotation(20)\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        mask = Image.open(self.mask_paths[idx]).convert(\"RGB\")\n\n        if self.augment:\n            seed = np.random.randint(2147483647)\n            torch.manual_seed(seed)\n            img = self.aug(img)\n            torch.manual_seed(seed)\n            mask = self.aug(mask)\n\n        img = self.transform(img)\n        mask = self.transform(mask).permute(1, 2, 0).numpy()\n\n        class_mask = np.zeros((IMAGE_SIZE[1], IMAGE_SIZE[0]), dtype=np.int64)\n        for rgb, cls in rgb_to_class.items():\n            class_mask[np.all(mask == np.array(rgb)/255.0, axis=-1)] = cls\n\n        return img, torch.tensor(class_mask, dtype=torch.long)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Visualization =====================\ndef visualize_prediction(image, pred_mask, true_mask):\n    image = image.permute(1, 2, 0).cpu().numpy()\n    pred_mask_rgb = np.zeros_like(image)\n    true_mask_rgb = np.zeros_like(image)\n\n    for cls, rgb in class_to_rgb.items():\n        pred_mask_rgb[pred_mask == cls] = rgb\n        true_mask_rgb[true_mask == cls] = rgb\n\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].imshow(image)\n    axs[0].set_title(\"Image\")\n    axs[1].imshow(pred_mask_rgb)\n    axs[1].set_title(\"Prediction\")\n    axs[2].imshow(true_mask_rgb)\n    axs[2].set_title(\"Ground Truth\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Train Function =====================\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, name):\n    best_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = 0\n        train_acc = MulticlassAccuracy(num_classes=NUM_CLASSES).to(\"cpu\")\n        train_iou = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(\"cpu\")\n\n        for img, mask in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n            img, mask = img.to(DEVICE), mask.to(DEVICE)\n            optimizer.zero_grad()\n            output = model(img)\n            if isinstance(output, dict):\n                output = F.interpolate(output[\"logits\"], size=IMAGE_SIZE, mode=\"bilinear\", align_corners=False)\n            loss = criterion(output, mask)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n            # Resize predictions to match ground truth\n            preds = torch.argmax(output, dim=1)\n            if preds.shape[1:] != mask.shape[1:]:\n                preds = F.interpolate(preds.unsqueeze(1).float(), size=mask.shape[1:], mode='nearest').squeeze(1).long()\n            preds = preds.detach().cpu()\n\n            train_acc.update(preds, mask.cpu())\n            train_iou.update(preds, mask.cpu())\n\n\n        model.eval()\n        val_loss = 0\n        val_acc = MulticlassAccuracy(num_classes=NUM_CLASSES).to(\"cpu\")\n        val_iou = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(\"cpu\")\n\n        with torch.no_grad():\n            for img, mask in val_loader:\n                img, mask = img.to(DEVICE), mask.to(DEVICE)\n                output = model(img)\n                if isinstance(output, dict):\n                    output = output[\"logits\"]\n                loss = criterion(output, mask)\n                val_loss += loss.item()\n\n                # Resize predictions to match ground truth\n                preds = torch.argmax(output, dim=1)\n                if preds.shape[1:] != mask.shape[1:]:\n                    preds = F.interpolate(preds.unsqueeze(1).float(), size=mask.shape[1:], mode='nearest').squeeze(1).long()\n                preds = preds.detach().cpu()\n\n                val_acc.update(preds, mask.cpu())\n                val_iou.update(preds, mask.cpu())\n\n        scheduler.step(val_loss)\n\n        print(f\"\\nEpoch {epoch+1}:\")\n        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc.compute():.4f}, mIoU: {train_iou.compute():.4f}\")\n        print(f\"  Val   Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc.compute():.4f}, mIoU: {val_iou.compute():.4f}\")\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), f\"{name}_best.pth\")\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(\"Early stopping\")\n                break","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Load Dataset =====================\ndeepglobe_dir = \"/kaggle/input/deepglobe-land-cover-classification-dataset/train\"\ndeepglobe_images = sorted(glob(os.path.join(deepglobe_dir, '*_sat.jpg')))\ndeepglobe_masks = sorted(glob(os.path.join(deepglobe_dir, '*_mask.png')))\n\ndubai_dir = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"\ndubai_images, dubai_masks = [], []\nfor tile in sorted(os.listdir(dubai_dir)):\n    tile_path = os.path.join(dubai_dir, tile)\n    if not os.path.isdir(tile_path): continue\n    img_folder = os.path.join(tile_path, \"images\")\n    mask_folder = os.path.join(tile_path, \"masks\")\n    dubai_images.extend(sorted(glob(os.path.join(img_folder, '*.jpg'))))\n    dubai_masks.extend(sorted(glob(os.path.join(mask_folder, '*.png'))))\n\nall_images = deepglobe_images + dubai_images\nall_masks = deepglobe_masks + dubai_masks\ntrain_imgs, val_imgs, train_masks, val_masks = train_test_split(all_images, all_masks, test_size=0.2, random_state=42)\n\ntrain_dataset = SatelliteDataset(train_imgs, train_masks, augment=True)\nval_dataset = SatelliteDataset(val_imgs, val_masks)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== Train SegFormer-B3 =====================\nsegformer = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b3-finetuned-ade-512-512\",\n    num_labels=NUM_CLASSES,\n    ignore_mismatched_sizes=True\n).to(DEVICE)\n\nsegformer_optimizer = optim.AdamW(segformer.parameters(), lr=LEARNING_RATE)\nsegformer_scheduler = optim.lr_scheduler.ReduceLROnPlateau(segformer_optimizer, mode='min', factor=0.5, patience=2, verbose=True)\ntrain_model(segformer, train_loader, val_loader, combined_loss, segformer_optimizer, segformer_scheduler, name=\"segformer\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# ===================== Evaluation =====================\nsegformer.load_state_dict(torch.load(\"segformer_best.pth\"))\nsegformer.eval()\n\n# Show 15 predictions\nnum_samples = 15\n\nfor i in range(num_samples):\n    sample_img, sample_mask = val_dataset[i]\n\n    with torch.no_grad():\n        pred_segformer = segformer(sample_img.unsqueeze(0).to(DEVICE))\n        if isinstance(pred_segformer, dict):\n            pred_segformer = pred_segformer[\"logits\"]\n\n        # Upsample to original size (512x512)\n        pred_segformer = F.interpolate(pred_segformer, size=(512, 512), mode='bilinear', align_corners=False)\n        pred_segformer = pred_segformer.argmax(1).squeeze().cpu()\n\n    visualize_prediction(sample_img, pred_segformer, sample_mask)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T03:02:28.244Z"}},"outputs":[],"execution_count":null}]}