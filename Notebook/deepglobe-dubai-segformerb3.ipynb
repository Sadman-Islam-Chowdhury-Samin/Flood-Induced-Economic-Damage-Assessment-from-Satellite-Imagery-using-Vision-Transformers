{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1196732,"sourceType":"datasetVersion","datasetId":681625},{"sourceId":1635643,"sourceType":"datasetVersion","datasetId":966962}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom PIL import Image\nfrom glob import glob\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:10:51.123389Z","iopub.execute_input":"2025-04-23T09:10:51.124142Z","iopub.status.idle":"2025-04-23T09:11:16.556968Z","shell.execute_reply.started":"2025-04-23T09:10:51.124109Z","shell.execute_reply":"2025-04-23T09:11:16.556424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Config ---\nNUM_CLASSES = 12\nIMAGE_SIZE = (512, 512)\nBATCH_SIZE = 4\nEPOCHS = 30\nPATIENCE = 5\nLEARNING_RATE = 2e-4\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.558056Z","iopub.execute_input":"2025-04-23T09:11:16.558576Z","iopub.status.idle":"2025-04-23T09:11:16.563719Z","shell.execute_reply.started":"2025-04-23T09:11:16.558556Z","shell.execute_reply":"2025-04-23T09:11:16.562973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RGB to Class Mapping\nrgb_to_class = {\n    (0, 255, 255): 0,     # urbanland\n    (255, 255, 0): 1,     # agricultureland\n    (255, 0, 255): 2,     # rangeland\n    (0, 255, 0): 3,       # forestland\n    (0, 0, 255): 4,       # water\n    (255, 255, 255): 5,   # barrenland\n    (0, 0, 0): 6,         # unknown\n    (60, 16, 152): 7,     # building\n    (132, 41, 246): 8,    # land_unpaved\n    (110, 193, 228): 9,   # road\n    (254, 221, 58): 10,   # vegetation_dubai\n    (155, 155, 155): 11   # unlabeled\n}\n\n# Class to RGB Mapping for visualization\nclass_to_rgb = {v: k for k, v in rgb_to_class.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.564494Z","iopub.execute_input":"2025-04-23T09:11:16.564987Z","iopub.status.idle":"2025-04-23T09:11:16.582577Z","shell.execute_reply.started":"2025-04-23T09:11:16.564963Z","shell.execute_reply":"2025-04-23T09:11:16.581912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rgb_mask_to_class(mask):\n    \"\"\"Convert RGB mask to 2D class index mask.\"\"\"\n    mask = np.array(mask)\n    class_mask = np.zeros(mask.shape[:2], dtype=np.uint8)\n    for rgb, idx in rgb_to_class.items():\n        class_mask[(mask == rgb).all(axis=-1)] = idx\n    return class_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.583287Z","iopub.execute_input":"2025-04-23T09:11:16.583507Z","iopub.status.idle":"2025-04-23T09:11:16.596045Z","shell.execute_reply.started":"2025-04-23T09:11:16.583484Z","shell.execute_reply":"2025-04-23T09:11:16.59529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DeepGlobe Paths\ndeepglobe_dir = \"/kaggle/input/deepglobe-land-cover-classification-dataset/train\"\n\n# Dubai Paths (multi-folder format)\ndubai_dir = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"\n\n# Load DeepGlobe images and masks\ndeepglobe_images = sorted(glob(os.path.join(deepglobe_dir, \"*.jpg\")))\ndeepglobe_masks = sorted(glob(os.path.join(deepglobe_dir, \"*.png\")))\n\n# Load Dubai images and masks from tile folders\ndubai_images, dubai_masks = [], []\nfor tile in sorted(os.listdir(dubai_dir)):\n    tile_path = os.path.join(dubai_dir, tile)\n    if not os.path.isdir(tile_path):\n        continue\n    img_folder = os.path.join(tile_path, \"images\")\n    mask_folder = os.path.join(tile_path, \"masks\")\n    dubai_images.extend(sorted(glob(os.path.join(img_folder, '*.jpg'))))\n    dubai_masks.extend(sorted(glob(os.path.join(mask_folder, '*.png'))))\n\nprint(f\"Loaded {len(deepglobe_images)} DeepGlobe images\")\nprint(f\"Loaded {len(dubai_images)} Dubai images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.59777Z","iopub.execute_input":"2025-04-23T09:11:16.597978Z","iopub.status.idle":"2025-04-23T09:11:16.768765Z","shell.execute_reply.started":"2025-04-23T09:11:16.597963Z","shell.execute_reply":"2025-04-23T09:11:16.768257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine both datasets\nall_images = deepglobe_images + dubai_images\nall_masks = deepglobe_masks + dubai_masks\nsources = ['deepglobe'] * len(deepglobe_images) + ['dubai'] * len(dubai_images)\n\n# Train/Val Split (Stratified)\ntrain_imgs, val_imgs, train_masks, val_masks, train_sources, val_sources = train_test_split(\n    all_images, all_masks, sources, test_size=0.2, stratify=sources, random_state=42\n)\n\n# Utility to print split info\ndef print_split_info(name, source_list):\n    counts = Counter(source_list)\n    print(f\"{name} Set:\")\n    for dataset, count in counts.items():\n        print(f\"  {dataset}: {count} images\")\n    print(f\"  Total: {len(source_list)} images\\n\")\n\nprint_split_info(\"Train\", train_sources)\nprint_split_info(\"Validation\", val_sources)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.769383Z","iopub.execute_input":"2025-04-23T09:11:16.769622Z","iopub.status.idle":"2025-04-23T09:11:16.781866Z","shell.execute_reply.started":"2025-04-23T09:11:16.769607Z","shell.execute_reply":"2025-04-23T09:11:16.781177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n        mask = Image.open(self.mask_paths[idx]).convert('RGB')\n        mask = rgb_mask_to_class(mask)\n\n        if self.transform:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        else:\n            img = T.ToTensor()(img)\n\n        return img.float(), mask.long()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.782612Z","iopub.execute_input":"2025-04-23T09:11:16.782899Z","iopub.status.idle":"2025-04-23T09:11:16.793235Z","shell.execute_reply.started":"2025-04-23T09:11:16.782873Z","shell.execute_reply":"2025-04-23T09:11:16.792664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Albumentations transforms\ntrain_transform = A.Compose([\n    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1]),\n    A.HorizontalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1]),\n    A.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.793977Z","iopub.execute_input":"2025-04-23T09:11:16.794193Z","iopub.status.idle":"2025-04-23T09:11:16.813474Z","shell.execute_reply.started":"2025-04-23T09:11:16.794177Z","shell.execute_reply":"2025-04-23T09:11:16.812749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = SegmentationDataset(train_imgs, train_masks, transform=train_transform)\nval_dataset = SegmentationDataset(val_imgs, val_masks, transform=val_transform)\n\n# Weighted sampling to balance smaller Dubai dataset\nsource_counts = Counter(train_sources)\nweights = [1.0 / source_counts[src] for src in train_sources]\ntrain_sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.814396Z","iopub.execute_input":"2025-04-23T09:11:16.814656Z","iopub.status.idle":"2025-04-23T09:11:16.821326Z","shell.execute_reply.started":"2025-04-23T09:11:16.814636Z","shell.execute_reply":"2025-04-23T09:11:16.820641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Hugging Face Segformer-B3 model\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b3-finetuned-ade-512-512\",\n    num_labels=NUM_CLASSES,\n    ignore_mismatched_sizes=True\n).to(DEVICE)\n\n# Feature extractor (optional – you’re not using it directly now, but keep it for completeness)\nfeature_extractor = SegformerFeatureExtractor(do_reduce_labels=False, size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:16.822065Z","iopub.execute_input":"2025-04-23T09:11:16.822309Z","iopub.status.idle":"2025-04-23T09:11:19.170064Z","shell.execute_reply.started":"2025-04-23T09:11:16.822291Z","shell.execute_reply":"2025-04-23T09:11:19.169345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:19.171011Z","iopub.execute_input":"2025-04-23T09:11:19.1713Z","iopub.status.idle":"2025-04-23T09:11:19.508645Z","shell.execute_reply.started":"2025-04-23T09:11:19.171276Z","shell.execute_reply":"2025-04-23T09:11:19.507789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, preds, targets):\n        preds = torch.softmax(preds, dim=1)\n        targets_one_hot = torch.nn.functional.one_hot(targets, NUM_CLASSES).permute(0, 3, 1, 2).float()\n\n        intersection = (preds * targets_one_hot).sum(dim=(2, 3))\n        union = preds.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n\n        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n        return 1 - dice.mean()\n        \nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, preds, targets):\n        logpt = -torch.nn.functional.cross_entropy(preds, targets, reduction='none')\n        pt = torch.exp(logpt)\n        loss = -((1 - pt) ** self.gamma) * logpt\n        return loss.mean()\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.dice = DiceLoss()\n        self.focal = FocalLoss(alpha, gamma)\n\n    def forward(self, preds, targets):\n        return self.dice(preds, targets) + self.focal(preds, targets)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:19.509721Z","iopub.execute_input":"2025-04-23T09:11:19.509949Z","iopub.status.idle":"2025-04-23T09:11:19.528698Z","shell.execute_reply.started":"2025-04-23T09:11:19.509933Z","shell.execute_reply":"2025-04-23T09:11:19.527907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(preds, targets, num_classes=NUM_CLASSES):\n    preds = torch.argmax(preds, dim=1).detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n\n    ious, dices, f1s = [], [], []\n    correct, total = 0, 0\n\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        true_cls = (targets == cls)\n\n        intersection = (pred_cls & true_cls).sum()\n        union = (pred_cls | true_cls).sum()\n        if union > 0:\n            iou = intersection / union\n            ious.append(iou)\n        else:\n            ious.append(np.nan)\n\n        dice = (2 * intersection) / (pred_cls.sum() + true_cls.sum() + 1e-6)\n        f1 = dice  # F1 = Dice in segmentation\n\n        dices.append(dice)\n        f1s.append(f1)\n\n        correct += (pred_cls == true_cls).sum()\n        total += true_cls.size\n\n    return {\n        \"Pixel Accuracy\": correct / total,\n        \"Mean IoU\": np.nanmean(ious),\n        \"Mean Dice\": np.nanmean(dices),\n        \"Mean F1\": np.nanmean(f1s)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:19.529751Z","iopub.execute_input":"2025-04-23T09:11:19.529939Z","iopub.status.idle":"2025-04-23T09:11:19.540225Z","shell.execute_reply.started":"2025-04-23T09:11:19.529925Z","shell.execute_reply":"2025-04-23T09:11:19.539172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, loss_fn):\n    model.train()\n    epoch_loss = 0\n    for imgs, masks in tqdm(loader, desc=\"Training\", leave=False):\n        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(pixel_values=imgs).logits\n        outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n        loss = loss_fn(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef validate_one_epoch(model, loader, loss_fn):\n    model.eval()\n    val_loss = 0\n    metrics_total = {\"Pixel Accuracy\": [], \"Mean IoU\": [], \"Mean Dice\": [], \"Mean F1\": []}\n\n    with torch.no_grad():\n        for imgs, masks in tqdm(loader, desc=\"Validation\", leave=False):\n            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n            outputs = model(pixel_values=imgs).logits\n            outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n            loss = loss_fn(outputs, masks)\n            val_loss += loss.item()\n\n            metrics = compute_metrics(outputs, masks)\n            for k in metrics:\n                metrics_total[k].append(metrics[k])\n\n    avg_metrics = {k: np.mean(v) for k, v in metrics_total.items()}\n    return val_loss / len(loader), avg_metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:19.543037Z","iopub.execute_input":"2025-04-23T09:11:19.543286Z","iopub.status.idle":"2025-04-23T09:11:19.558133Z","shell.execute_reply.started":"2025-04-23T09:11:19.54327Z","shell.execute_reply":"2025-04-23T09:11:19.557609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = CombinedLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5, verbose=True)\n\nbest_miou = 0\npatience_counter = 0\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_pixel_acc\": [],\n    \"val_miou\": [],\n    \"val_dice\": [],\n    \"val_f1\": []\n}\n\nfor epoch in range(1, EPOCHS + 1):\n    print(f\"\\n Epoch {epoch}/{EPOCHS}\")\n\n    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n    val_loss, val_metrics = validate_one_epoch(model, val_loader, loss_fn)\n\n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    for k, v in val_metrics.items():\n        print(f\"{k}: {v:.4f}\")\n\n    scheduler.step(val_loss)\n\n    # Save history\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_pixel_acc\"].append(val_metrics[\"Pixel Accuracy\"])\n    history[\"val_miou\"].append(val_metrics[\"Mean IoU\"])\n    history[\"val_dice\"].append(val_metrics[\"Mean Dice\"])\n    history[\"val_f1\"].append(val_metrics[\"Mean F1\"])\n\n    # Save best model\n    if val_metrics[\"Mean IoU\"] > best_miou:\n        best_miou = val_metrics[\"Mean IoU\"]\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(\"Best model saved!\")\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:11:19.558878Z","iopub.execute_input":"2025-04-23T09:11:19.559629Z","iopub.status.idle":"2025-04-23T15:38:42.698946Z","shell.execute_reply.started":"2025-04-23T09:11:19.559606Z","shell.execute_reply":"2025-04-23T15:38:42.6978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(history):\n    epochs = range(1, len(history[\"train_loss\"]) + 1)\n\n    plt.figure(figsize=(18, 12))\n\n    # Train vs Val Loss\n    plt.subplot(2, 3, 1)\n    plt.plot(epochs, history[\"train_loss\"], label='Train Loss')\n    plt.plot(epochs, history[\"val_loss\"], label='Val Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Pixel Accuracy\n    plt.subplot(2, 3, 2)\n    plt.plot(epochs, history[\"val_pixel_acc\"], label='Pixel Accuracy')\n    plt.title('Pixel Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Mean IoU\n    plt.subplot(2, 3, 3)\n    plt.plot(epochs, history[\"val_miou\"], label='Mean IoU')\n    plt.title('Mean IoU')\n    plt.xlabel('Epoch')\n    plt.ylabel('IoU')\n    plt.legend()\n\n    # Dice Score\n    plt.subplot(2, 3, 4)\n    plt.plot(epochs, history[\"val_dice\"], label='Mean Dice')\n    plt.title('Mean Dice')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice')\n    plt.legend()\n\n    # F1 Score\n    plt.subplot(2, 3, 5)\n    plt.plot(epochs, history[\"val_f1\"], label='F1 Score')\n    plt.title('F1 Score')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1 Score')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:38:42.700227Z","iopub.execute_input":"2025-04-23T15:38:42.70051Z","iopub.status.idle":"2025-04-23T15:38:42.708142Z","shell.execute_reply.started":"2025-04-23T15:38:42.700485Z","shell.execute_reply":"2025-04-23T15:38:42.707454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def class_mask_to_rgb(mask):\n    \"\"\"Convert a class-indexed mask to RGB.\"\"\"\n    h, w = mask.shape\n    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n    for class_idx, color in class_to_rgb.items():\n        rgb[mask == class_idx] = color\n    return rgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:38:42.708843Z","iopub.execute_input":"2025-04-23T15:38:42.709075Z","iopub.status.idle":"2025-04-23T15:38:42.724205Z","shell.execute_reply.started":"2025-04-23T15:38:42.70906Z","shell.execute_reply":"2025-04-23T15:38:42.723419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training performance\nplot_training_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:38:42.724887Z","iopub.execute_input":"2025-04-23T15:38:42.725083Z","iopub.status.idle":"2025-04-23T15:38:43.622671Z","shell.execute_reply.started":"2025-04-23T15:38:42.725068Z","shell.execute_reply":"2025-04-23T15:38:43.62193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iou_and_dice(pred, target, num_classes=NUM_CLASSES):\n    ious = []\n    dices = []\n    for cls in range(num_classes):\n        pred_cls = (pred == cls)\n        target_cls = (target == cls)\n        intersection = np.logical_and(pred_cls, target_cls).sum()\n        union = np.logical_or(pred_cls, target_cls).sum()\n        if union == 0:\n            iou = float('nan')\n        else:\n            iou = intersection / union\n        dice = (2 * intersection) / (pred_cls.sum() + target_cls.sum() + 1e-6)\n        ious.append(iou)\n        dices.append(dice)\n    return np.nanmean(ious), np.nanmean(dices)\n\n\ndef visualize_predictions(model, dataset, num_samples):\n    model.eval()\n    indices = random.sample(range(len(dataset)), num_samples)\n\n    plt.figure(figsize=(15, num_samples * 3))\n\n    for i, idx in enumerate(indices):\n        img, gt_mask = dataset[idx]\n        img_input = img.unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            output = model(pixel_values=img_input).logits\n            output = torch.nn.functional.interpolate(output, size=gt_mask.shape, mode='bilinear', align_corners=False)\n            pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n\n        # Compute metrics\n        mean_iou, mean_dice = calculate_iou_and_dice(pred_mask, gt_mask.numpy())\n\n        # Convert to RGB\n        rgb_gt = class_mask_to_rgb(gt_mask.numpy())\n        rgb_pred = class_mask_to_rgb(pred_mask)\n        img_np = img.permute(1, 2, 0).cpu().numpy()\n        img_np = (img_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n        img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n\n        # Show row\n        plt.subplot(num_samples, 3, i * 3 + 1)\n        plt.imshow(img_np)\n        plt.title(\"Input Image\")\n        plt.axis(\"off\")\n\n        plt.subplot(num_samples, 3, i * 3 + 2)\n        plt.imshow(rgb_gt)\n        plt.title(\"Ground Truth\")\n        plt.axis(\"off\")\n\n        plt.subplot(num_samples, 3, i * 3 + 3)\n        plt.imshow(rgb_pred)\n        plt.title(f\"Prediction\\nDice: {mean_dice:.2f} | IoU: {mean_iou:.2f}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:41:52.421264Z","iopub.execute_input":"2025-04-23T15:41:52.421558Z","iopub.status.idle":"2025-04-23T15:41:52.431854Z","shell.execute_reply.started":"2025-04-23T15:41:52.421536Z","shell.execute_reply":"2025-04-23T15:41:52.431041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show predictions\nvisualize_predictions(model, val_dataset, num_samples=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:43:13.360469Z","iopub.execute_input":"2025-04-23T15:43:13.360753Z","iopub.status.idle":"2025-04-23T15:43:51.705363Z","shell.execute_reply.started":"2025-04-23T15:43:13.360731Z","shell.execute_reply":"2025-04-23T15:43:51.704598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}